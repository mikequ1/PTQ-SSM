{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "def absmax_quantize(X):\n",
    "    # Calculate scale\n",
    "    scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = (scale * X).round()\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "def absmax_perchannel_quantize(X, channel_axis=0):\n",
    "    \n",
    "    absmax = torch.amax(torch.abs(X), dim=channel_axis, keepdim=True)\n",
    "    absmax[absmax == 0] = 1e-8  # avoid div by 0\n",
    "    scale = 127 / absmax\n",
    "    X_quant = (scale * X).round()\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "def tanh_quantize(X, k=3):\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "\n",
    "    # 3 sigma\n",
    "    X_clipped = torch.clamp(X, mean - k * std, mean + k * std)\n",
    "    X_normalized = (X_clipped - mean) / std\n",
    "\n",
    "    X_tanh = torch.tanh(X_normalized)\n",
    "    scale = 127 / torch.max(torch.abs(X_tanh))\n",
    "\n",
    "    X_quant = (scale * X_tanh).round()\n",
    "    X_dequant = X_quant / scale\n",
    "    X_dequant = torch.arctanh(X_dequant.clamp(-0.99, 0.99)) * std + mean\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import tqdm\n",
    "class TextGenerator:\n",
    "    def __init__(self, tokenizer, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def generate_text(self, model, input_text, max_length=50):\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt').to(self.device)\n",
    "        output = model.generate(inputs=input_ids,\n",
    "                                max_length=max_length,\n",
    "                                do_sample=True,\n",
    "                                top_k=30,\n",
    "                                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                                attention_mask=input_ids.new_ones(input_ids.shape))\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    def calculate_perplexity(self, model, text):\n",
    "        # Encode the text\n",
    "        encodings = self.tokenizer(text, return_tensors='pt').to(self.device)\n",
    "\n",
    "        # Define input_ids and target_ids\n",
    "        input_ids = encodings.input_ids\n",
    "        target_ids = input_ids.clone()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # Loss calculation\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "        # Perplexity calculation\n",
    "        ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "        return ppl\n",
    "\n",
    "class LambadaEvaluator:\n",
    "    def __init__(self, dataset, tokenizer, device):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        # tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            example = self.tokenizer(examples[\"text\"])\n",
    "            return example\n",
    "\n",
    "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        # The task is to predict the last word of the input.\n",
    "        total, hit = 0, 0\n",
    "        for batch in self.dataset:\n",
    "            input_ids = batch[\"input_ids\"].to(self.device).unsqueeze(0)\n",
    "            label = input_ids[:, -1]\n",
    "            outputs = model(input_ids)\n",
    "            last_token_logits = outputs.logits[:, -2, :]\n",
    "            pred = last_token_logits.argmax(dim=-1)\n",
    "            total += label.size(0)\n",
    "            hit += (pred == label).sum().item()\n",
    "        acc = hit / total\n",
    "        return acc\n",
    "    \n",
    "class WikitextEvaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=40):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        self.dataset = tokenizer(\n",
    "            \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        nlls = []\n",
    "        for i in tqdm.tqdm(range(self.n_samples), desc=\"Evaluating...\"):\n",
    "            batch = self.dataset[:, (i * 2048) : ((i + 1) * 2048)].to(model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = model(batch).logits\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.dataset[:, (i * 2048) : ((i + 1) * 2048)][:, 1:]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (self.n_samples * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikequ/miniconda3/envs/quantization/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 5,488,713,728 bytes\n",
      "Calculated model size: 5,488,713,728 bytes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set device to CPU for now\n",
    "device = 'cuda'\n",
    "\n",
    "# Load model and tokenizer\n",
    "# model_id = 'state-spaces/mamba-790m-hf'\n",
    "model_id = 'state-spaces/mamba-1.4b-hf'\n",
    "# model_id = 'state-spaces/mamba-2.8b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "dataset = load_dataset(\"lambada\", split=\"validation[:100]\")\n",
    "lambada_evaluator = LambadaEvaluator(dataset, tokenizer, \"cuda\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "wikitext_evaluator = WikitextEvaluator(dataset, tokenizer, \"cuda\")\n",
    "text_generator = TextGenerator(tokenizer, 'cuda')\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")\n",
    "\n",
    "total_memory = 0\n",
    "for param in model.parameters():\n",
    "    dtype_size = torch.finfo(param.dtype).bits // 8 # using int 8 quantization\n",
    "    total_memory += param.numel() * dtype_size\n",
    "\n",
    "print(f\"Calculated model size: {total_memory:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Mamba Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaForCausalLM(\n",
      "  (backbone): MambaModel(\n",
      "    (embeddings): Embedding(50280, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-47): 48 x MambaBlock(\n",
      "        (norm): MambaRMSNorm(2048, eps=1e-05)\n",
      "        (mixer): MambaMixer(\n",
      "          (conv1d): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(3,), groups=4096)\n",
      "          (act): SiLU()\n",
      "          (in_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (x_proj): Linear(in_features=4096, out_features=160, bias=False)\n",
      "          (dt_proj): Linear(in_features=128, out_features=4096, bias=True)\n",
      "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_f): MambaRMSNorm(2048, eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50280, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "The model first maps our input tokens (which has a vocabulary size of 50,280) to an embedding dimension of 2048. This will act as our input sentence sequence to the Mamba Model\n",
    "\n",
    "### 48 x Mamba Blocks\n",
    "Recall the state-space model update system of equations:   \n",
    "$\\frac{dx(t)}{dt} = Ax(t) + Bu(t)$      \n",
    "$y(t) = Cx(t) + Du(t)$    \n",
    "where $x(t) \\in \\mathbb{R}^H$ is the hidden state vector, $u(t) \\in \\mathbb{R}^N$ is the input vector, and $y(t) \\in \\mathbb{M}^N$ is the output vector\n",
    "\n",
    "We may discretize this as    \n",
    "$x_{t+1} = Ax_t + Bu_t$   \n",
    "$y_t = Cx_t + Du_t$\n",
    "\n",
    "These matrices should have the following dimensions: $A \\in \\mathbb{R}^{H \\times H}$, $B \\in \\mathbb{R}^{H \\times N}$, $C \\in \\mathbb{R}^{M \\times H}$, and $D \\in \\mathbb{R}^{M \\times N}$.\n",
    "\n",
    "Moving on to the implementation of the model: (here, we consult the Mamba's source code by the authors to understand how the model works)\n",
    "\n",
    "`in_proj` projects the input (a.k.a. `d_model=2048`) to a combined inner state dimension (a.k.a. `2 * d_inner=8192`). The idea behind projecting our input to a higher dimensional space is to allow for richer feature interactions and redundancies. It is important to note that the true state dimension `d_inner` is 4096. And that `in_proj`actually projects the input to the concatenation of the the state vector $x$ itself and a residual stream $z$ (hence the multiplication by 2). \n",
    "\n",
    "`conv1d` performs a simple 1d convolution on `d_inner`, which aims to blend local temporal dependencies in the input vector. \n",
    "\n",
    "`x_proj` projects the input vector we obtained into a space that is the concatenation of dimensions (`dt_rank`, `d_state`, `d_state`). What this projection does is that it maps the current input vector to `dt`, `B`, and `C`. `dt` here has dimension`dt_rank`, which refers to the rank used in the `dt_proj` layer. The intuition behind `dt_rank` is that it controls how complex the adjustments should be when it comes to modeling temporal dynamics of the sequence. This makes sense because for a higher `dt_rank`, we have more parameters in the linear layer that projects back up to `dt_inner` in the `dt_proj` layer. A similar intuition applies to `B`, `C` and their dimension `d_state`. Instead of using the original SSM formulation, where $B \\in \\mathbb{R}^{H \\times N = 4096 \\times 2048}$ and $C \\in \\mathbb{R}^{M \\times H = 2048 \\times 4096}$, $B$ (which determines how inputs affect state transitions), and $C$ (which controls how states are mapped to outputs) instead operates on a latent representation of the state vector, and this latent representation is obtained from the $A$ matrix, which, instead of being a $\\mathbb{R}^{H \\times H}$ matrix, maps from H=4096 to the latent space `d_state`. This is what makes mamba different from traditional SSM models, as $B$ and $C$ matrices can \"selectively scan\" for the context that can best predict the next token, depending on what the input is. The purpose of `x_proj` is therefore to generate the parameters in order to dynamically adjust the state vector.\n",
    "\n",
    "`dt_proj` projects `dt_rank` to `d_inner`. After obtaining the optimal parameters for state updates, `dt_proj` performs the status updates to update the state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.0093,  0.0195,  0.0045,  ..., -0.0280,  0.0205,  0.0303],\n",
      "        [-0.0455, -0.0419, -0.0181,  ...,  0.0200, -0.0017, -0.0090],\n",
      "        [ 0.0152, -0.0286,  0.0063,  ..., -0.0122, -0.0423, -0.0070],\n",
      "        ...,\n",
      "        [ 0.0288, -0.0310,  0.0342,  ...,  0.0013, -0.0933, -0.0034],\n",
      "        [ 0.0006,  0.0376, -0.0062,  ...,  0.0058,  0.0233, -0.0073],\n",
      "        [ 0.0035, -0.0039, -0.0507,  ..., -0.0188, -0.0253,  0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "Absmax quantized weights:\n",
      "tensor([[ -1,   3,   1,  ...,  -4,   3,   4],\n",
      "        [ -6,  -6,  -3,  ...,   3,   0,  -1],\n",
      "        [  2,  -4,   1,  ...,  -2,  -6,  -1],\n",
      "        ...,\n",
      "        [  4,  -4,   5,  ...,   0, -13,   0],\n",
      "        [  0,   5,  -1,  ...,   1,   3,  -1],\n",
      "        [  0,  -1,  -7,  ...,  -3,  -4,   2]], device='cuda:0',\n",
      "       dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.backbone.layers[0].mixer.in_proj.weight\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, _ = absmax_quantize(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLAMBADA Accuracy Evaluation\")\n",
    "acc_original = lambada_evaluator.evaluate(model)\n",
    "print(f\"accuracy on LAMBADA: {acc_original}\")\n",
    "\n",
    "print(\"\\nPerplexity Evaluation on WikiText\")\n",
    "pp_wikitext = wikitext_evaluator.evaluate(model)\n",
    "print(f'perplexity on wikitext: {pp_wikitext}')\n",
    "\n",
    "print(\"\\nPerplexity Evaluation on custom text\")\n",
    "original_text = text_generator.generate_text(model, \"I have a dream\")\n",
    "print(f\"output text:\\n{original_text}\")\n",
    "ppl = text_generator.calculate_perplexity(model, original_text)\n",
    "print(f\"original model perplexity: {ppl.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# cur_model = \"w8_all\"\n",
    "# cur_model = \"w8_pc_all\"\n",
    "# cur_model = \"w8_tanh_all\"\n",
    "cur_model = \"w8_inout\"\n",
    "# cur_model = \"w8_pc_inout\"\n",
    "# cur_model = \"w8_tanh_inout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W8_All: Quantizing all weights of the Mamba Model naively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "if (cur_model == \"w8_all\"):\n",
    "    model_abs = deepcopy(model)\n",
    "    model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    total_memory = 0\n",
    "    for param in model_abs.parameters():\n",
    "        _, dequantized = absmax_quantize(param.data)\n",
    "        param.data = dequantized\n",
    "        dtype_size = 8 // 8\n",
    "        total_memory += param.numel() * dtype_size\n",
    "\n",
    "    print(f\"Calculated model size: {total_memory:,} bytes\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "if (cur_model == \"w8_all\"):\n",
    "    print(\"\\nLAMBADA Accuracy Evaluation\")\n",
    "    acc_original = lambada_evaluator.evaluate(model_abs)\n",
    "    print(f\"accuracy on LAMBADA: {acc_original}\")\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on WikiText\")\n",
    "    pp_wikitext = wikitext_evaluator.evaluate(model_abs)\n",
    "    print(f'perplexity on wikitext: {pp_wikitext}')\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on custom text\")\n",
    "    original_text = text_generator.generate_text(model_abs, \"I have a dream\")\n",
    "    print(f\"output text:\\n{original_text}\")\n",
    "    ppl = text_generator.calculate_perplexity(model_abs, original_text)\n",
    "    print(f\"quantized model perplexity: {ppl.item():.2f}\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W8_PC_All: Naive per-channel quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "if (cur_model == \"w8_pc_all\"):\n",
    "    model_pcabs = deepcopy(model)\n",
    "    model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    total_memory = 0\n",
    "    for param in model_pcabs.parameters():\n",
    "        _, dequantized = absmax_perchannel_quantize(param.data)\n",
    "        param.data = dequantized\n",
    "        dtype_size = 8 // 8\n",
    "        total_memory += param.numel() * dtype_size\n",
    "\n",
    "    print(f\"Calculated model size: {total_memory:,} bytes\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "if (cur_model == \"w8_pc_all\"):\n",
    "    print(\"\\nLAMBADA Accuracy Evaluation\")\n",
    "    acc_original = lambada_evaluator.evaluate(model_pcabs)\n",
    "    print(f\"accuracy on LAMBADA: {acc_original}\")\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on WikiText\")\n",
    "    pp_wikitext = wikitext_evaluator.evaluate(model_pcabs)\n",
    "    print(f'perplexity on wikitext: {pp_wikitext}')\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on custom text\")\n",
    "    original_text = text_generator.generate_text(model_pcabs, \"I have a dream\")\n",
    "    print(f\"output text:\\n{original_text}\")\n",
    "    ppl = text_generator.calculate_perplexity(model_pcabs, original_text)\n",
    "    print(f\"quantized model perplexity: {ppl.item():.2f}\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W8_Tanh_All: Using Tanh-Based function due to the normal distribution of weights to reduce ambiguity and quantization error around mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "if (cur_model == \"w8_tanh_all\"):\n",
    "    model_tanh = deepcopy(model)\n",
    "\n",
    "    total_memory = 0\n",
    "    for param in model_tanh.parameters():\n",
    "        _, dequantized = tanh_quantize(param.data)\n",
    "        param.data = dequantized\n",
    "        dtype_size = 8 // 8\n",
    "        total_memory += param.numel() * dtype_size\n",
    "\n",
    "    print(f\"Calculated model size: {total_memory:,} bytes\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we do this? let's visuzlize\n",
    "\n",
    "if (cur_model == \"w8_tanh_all\"):\n",
    "    i = 47\n",
    "    import matplotlib.pyplot as plt\n",
    "    weights_og_inproj = model.backbone.layers[i].mixer.in_proj.weight.cpu().detach().numpy().flatten()\n",
    "    weights_og_outproj = model.backbone.layers[i].mixer.out_proj.weight.cpu().detach().numpy().flatten()\n",
    "    weights_tanh_inproj = model_tanh.backbone.layers[i].mixer.in_proj.weight.cpu().detach().numpy().flatten()\n",
    "    weights_tanh_outproj = model_tanh.backbone.layers[i].mixer.out_proj.weight.cpu().detach().numpy().flatten()\n",
    "    weights_og_xproj = model.backbone.layers[i].mixer.x_proj.weight.cpu().detach().numpy().flatten()\n",
    "    weights_og_dtproj = model.backbone.layers[i].mixer.dt_proj.weight.cpu().detach().numpy().flatten()\n",
    "    weights_tanh_xproj = model_tanh.backbone.layers[i].mixer.x_proj.weight.cpu().detach().numpy().flatten()\n",
    "    weights_tanh_dtproj = model_tanh.backbone.layers[i].mixer.dt_proj.weight.cpu().detach().numpy().flatten()\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(9,6), sharex=True)\n",
    "\n",
    "    axs[0][0].hist(weights_og_inproj, bins=150, alpha=0.5, label='in_proj original weights', color='blue', range=(-0.2, 0.2))\n",
    "    axs[0][0].hist(weights_tanh_inproj, bins=150, alpha=0.5, label='in_proj quantized weights', color='red', range=(-0.2, 0.2))\n",
    "    axs[0][1].hist(weights_og_outproj, bins=150, alpha=0.5, label='out_proj original weights', color='blue', range=(-0.2, 0.2))\n",
    "    axs[0][1].hist(weights_tanh_outproj, bins=150, alpha=0.5, label='out_proj quantized weights', color='green', range=(-0.2, 0.2))\n",
    "    axs[1][0].hist(weights_og_xproj, bins=150, alpha=0.5, label='x_proj original weights', color='blue', range=(-0.2, 0.2))\n",
    "    axs[1][0].hist(weights_tanh_xproj, bins=150, alpha=0.5, label='x_proj quantized weights', color='red', range=(-0.2, 0.2))\n",
    "    axs[1][1].hist(weights_og_dtproj, bins=150, alpha=0.5, label='dt_proj original weights', color='blue', range=(-0.2, 0.2))\n",
    "    axs[1][1].hist(weights_tanh_dtproj, bins=150, alpha=0.5, label='dt_proj quantized weights', color='green', range=(-0.2, 0.2))\n",
    "\n",
    "    # Add grid\n",
    "    for row in axs:\n",
    "        for ax in row:\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            ax.set_xlabel('Weights', fontsize=14)\n",
    "            ax.set_ylabel('Count', fontsize=14)\n",
    "            ax.legend()\n",
    "\n",
    "    axs[0][0].set_title('in_proj layer', fontsize=16)\n",
    "    axs[0][1].set_title('out_proj layer', fontsize=16)\n",
    "    axs[1][0].set_title('x_proj layer', fontsize=16)\n",
    "    axs[1][1].set_title('dt_proj layer', fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does sigmoid quantization do?\n",
    "\n",
    "if (cur_model == \"w8_tanh_all\"):\n",
    "    quantized_in_proj = tanh_quantize(model.backbone.layers[0].mixer.in_proj.weight.cpu())[0].detach().numpy().flatten()\n",
    "    quantized_out_proj = tanh_quantize(model.backbone.layers[0].mixer.out_proj.weight.cpu())[0].detach().numpy().flatten()\n",
    "\n",
    "    fig, axs = plt.subplots(2, figsize=(6,4), sharex=True)\n",
    "\n",
    "    axs[0].hist(quantized_in_proj, bins=150, alpha=0.5, label='in_proj original weights', color='blue', range=(-192, 192))\n",
    "    axs[1].hist(quantized_out_proj, bins=150, alpha=0.5, label='out_proj original weights', color='blue', range=(-192, 192))\n",
    "\n",
    "    # Add grid\n",
    "    for ax in axs:\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    axs[0].set_title('Comparison of Original and Absmax Quantized Weights', fontsize=16)\n",
    "    axs[1].set_title('Comparison of Original and Zeropoint Quantized Weights', fontsize=16)\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('Weights', fontsize=14)\n",
    "        ax.set_ylabel('Count', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "if (cur_model == \"w8_tanh_all\"):\n",
    "    print(\"\\nLAMBADA Accuracy Evaluation\")\n",
    "    acc_original = lambada_evaluator.evaluate(model_tanh)\n",
    "    print(f\"accuracy on LAMBADA: {acc_original}\")\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on WikiText\")\n",
    "    pp_wikitext = wikitext_evaluator.evaluate(model_tanh)\n",
    "    print(f'perplexity on wikitext: {pp_wikitext}')\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on custom text\")\n",
    "    original_text = text_generator.generate_text(model_tanh, \"I have a dream\")\n",
    "    print(f\"output text:\\n{original_text}\")\n",
    "    ppl = text_generator.calculate_perplexity(model_tanh, original_text)\n",
    "    print(f\"quantized model perplexity: {ppl.item():.2f}\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W8_InOut: Quantizing only in_proj and out_proj\n",
    "Why only in_proj and out_proj? These two projections expand and compress our inputs to and back from a richer, higher dimensional space. This is because higher dimensional spaces are used to model lots of feature interactions among inputs. This makes them not as sensitive to quantization errors due to potential redundancies in the high-D representation.\n",
    "\n",
    "Furthermore, in_proj and out_proj account for the majority of parameters in the model, as they are both used for projecting to and from the input embedding (2048) and some higher-dimensional state space (4096 and 8192). In contrast, x_proj and dt_proj, which enables state updates and are the backbone of the model, only operate on a 16-dimensional low-rank state space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated model size: 1,864,835,072 bytes\n",
      "Proportion of in_proj and out_proj params: 0.8803225031305549\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "if (cur_model == \"w8_inout\"):\n",
    "    model_w8inout = deepcopy(model)\n",
    "    model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    total_memory = 0\n",
    "    num_in_out_params = 0\n",
    "    num_total_params = 0\n",
    "    for name, param in model_w8inout.named_parameters():\n",
    "        # print(name)\n",
    "        if \"in_proj\" in name or \"out_proj\" in name:\n",
    "            _, dequantized = absmax_quantize(param.data)\n",
    "            param.data = dequantized\n",
    "            dtype_size = 8 // 8\n",
    "            total_memory += param.numel() * dtype_size\n",
    "            num_in_out_params += param.numel()\n",
    "        else:\n",
    "            dtype_size = torch.finfo(param.dtype).bits // 8\n",
    "            total_memory += param.numel() * dtype_size\n",
    "        num_total_params += param.numel()\n",
    "\n",
    "    print(f\"Calculated model size: {total_memory:,} bytes\")\n",
    "    print(f\"Proportion of in_proj and out_proj params: {num_in_out_params/num_total_params}\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAMBADA Accuracy Evaluation\n",
      "accuracy on LAMBADA: 0.82\n",
      "\n",
      "Perplexity Evaluation on WikiText\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████| 40/40 [01:48<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity on wikitext: 10.558557510375977\n",
      "\n",
      "Perplexity Evaluation on custom text\n",
      "output text:\n",
      "I have a dream, I have a vision.\" \"So I'll ask the question again, as a prefect you're expected to attend the council every day.\" \"That's very good.\" \"And, as you will see, your mother and\n",
      "quantized model perplexity: 14.90\n"
     ]
    }
   ],
   "source": [
    "if (cur_model == \"w8_inout\"):\n",
    "    print(\"\\nLAMBADA Accuracy Evaluation\")\n",
    "    acc_original = lambada_evaluator.evaluate(model_w8inout)\n",
    "    print(f\"accuracy on LAMBADA: {acc_original}\")\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on WikiText\")\n",
    "    pp_wikitext = wikitext_evaluator.evaluate(model_w8inout)\n",
    "    print(f'perplexity on wikitext: {pp_wikitext}')\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on custom text\")\n",
    "    original_text = text_generator.generate_text(model_w8inout, \"I have a dream\")\n",
    "    print(f\"output text:\\n{original_text}\")\n",
    "    ppl = text_generator.calculate_perplexity(model_w8inout, original_text)\n",
    "    print(f\"quantized model perplexity: {ppl.item():.2f}\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W8_PC_InOut: Per-Channel Quantization of only in_proj and out_proj layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "if (cur_model == \"w8_pc_inout\"):\n",
    "    model_w8_pc_inout = deepcopy(model)\n",
    "    model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    total_memory = 0\n",
    "\n",
    "    for name, param in model_w8_pc_inout.named_parameters():\n",
    "        # print(name)\n",
    "        if \"in_proj\" in name or \"out_proj\" in name:\n",
    "            _, dequantized = absmax_perchannel_quantize(param.data)\n",
    "            param.data = dequantized\n",
    "            dtype_size = 8 // 8\n",
    "            total_memory += param.numel() * dtype_size\n",
    "        else:\n",
    "            dtype_size = torch.finfo(param.dtype).bits // 8\n",
    "            total_memory += param.numel() * dtype_size\n",
    "\n",
    "    print(f\"Calculated model size: {total_memory:,} bytes\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "if (cur_model == \"w8_pc_inout\"):\n",
    "    print(\"\\nLAMBADA Accuracy Evaluation\")\n",
    "    acc_original = lambada_evaluator.evaluate(model_w8_pc_inout)\n",
    "    print(f\"accuracy on LAMBADA: {acc_original}\")\n",
    "\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on WikiText\")\n",
    "    pp_wikitext = wikitext_evaluator.evaluate(model_w8_pc_inout)\n",
    "    print(f'perplexity on wikitext: {pp_wikitext}')\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on custom text\")\n",
    "    original_text = text_generator.generate_text(model_w8_pc_inout, \"I have a dream\")\n",
    "    print(f\"output text:\\n{original_text}\")\n",
    "    ppl = text_generator.calculate_perplexity(model_w8_pc_inout, original_text)\n",
    "    print(f\"quantized model perplexity: {ppl.item():.2f}\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W8_Tanh_InOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "if (cur_model == \"w8_tanh_inout\"):\n",
    "    model_w8_tanh_inout = deepcopy(model)\n",
    "\n",
    "    total_memory = 0\n",
    "    \n",
    "    for name, param in model_w8_tanh_inout.named_parameters():\n",
    "        # print(name)\n",
    "        if \"in_proj\" in name or \"out_proj\" in name:\n",
    "            _, dequantized = tanh_quantize(param.data)\n",
    "            param.data = dequantized\n",
    "            dtype_size = 8 // 8\n",
    "            total_memory += param.numel() * dtype_size\n",
    "        else:\n",
    "            dtype_size = torch.finfo(param.dtype).bits // 8\n",
    "            total_memory += param.numel() * dtype_size\n",
    "\n",
    "    print(f\"Calculated model size: {total_memory:,} bytes\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n"
     ]
    }
   ],
   "source": [
    "if (cur_model == \"w8_tanh_inout\"):\n",
    "    print(\"\\nLAMBADA Accuracy Evaluation\")\n",
    "    acc_original = lambada_evaluator.evaluate(model_w8_tanh_inout)\n",
    "    print(f\"accuracy on LAMBADA: {acc_original}\")\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on WikiText\")\n",
    "    pp_wikitext = wikitext_evaluator.evaluate(model_w8_tanh_inout)\n",
    "    print(f'perplexity on wikitext: {pp_wikitext}')\n",
    "\n",
    "    print(\"\\nPerplexity Evaluation on custom text\")\n",
    "    original_text = text_generator.generate_text(model_w8_tanh_inout, \"I have a dream\")\n",
    "    print(f\"output text:\\n{original_text}\")\n",
    "    ppl = text_generator.calculate_perplexity(model_w8_tanh_inout, original_text)\n",
    "    print(f\"quantized model perplexity: {ppl.item():.2f}\")\n",
    "else:\n",
    "    print(\"skipped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
